{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DXC Industrialized AI Starter DXC Indusrialized AI Starter makes it easy for you to deploy your AI algorithms (Industrialize). If you are a data scientist, working on an algorithm that you would like to deploy across the enterprise, DXC's Industrialized AI starter makes it easier for you to: Access, clean, and explore raw data Build data pipelines Run AI experiments Publish microservices Here is short visual represtion about DXC Indusrialized AI Starter. Installation In order to install and use the DXC AI Starter library, please use the below code snippet: 1. pip install DXC-Industrialized-AI-Starter 2. from dxc import ai Getting Started Access, Clean, and Explore Raw Data Use the library to access, clean, and explore your raw data. #Access raw data df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) df = ai.read_data_frame_from_local_json() df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() #Clean data: Imputes missing data, removes empty rows and columns, anonymizes text. raw_data = ai.clean_dataframe(df) #Explore raw data: ai.visualize_missing_data(raw_data) #visualizes relationships between all features in data. ai.explore_features(raw_data) #creates a visual display of missing data. ai.plot_distributions(raw_data) #creates a distribution graph for each column. Click here for details about Acess,clean,explore raw data. Build Data Pipelines Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. In order to get started, you need to first have an MongoDB account which you can signup for free and create a database \"connection_string\" and specify those details in the data_layer below. The following code connects to MongoDB and stores raw data for processing. #Insert data into MongoDB: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) Once raw data is stored, you can run pipelines to transform the data. This code instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) #refined data will be stored in pandas dataframe. Click here for details about building data pipeline. Run AI Experiments Use the DXC AI Starter to build and test algorithms. This code executes an experiment by running run_experiment() on an experiment design. experiment_design = { #model options include ['regression()', 'classification()'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) Click here for details about run AI experiments. Publish Microservice The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. You must create an Algorithmia account to use. Below is the example for publishing a microservice. #trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } #publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) Click here for details about publishing microservice. Example Here is an detailed and in-depth example of colab notebook for DXC Indusrialized AI Starter library usage. Reporting Issues If you find any issues, feel free to report them here with clear description of your issue.","title":"Welcome"},{"location":"#dxc-industrialized-ai-starter","text":"DXC Indusrialized AI Starter makes it easy for you to deploy your AI algorithms (Industrialize). If you are a data scientist, working on an algorithm that you would like to deploy across the enterprise, DXC's Industrialized AI starter makes it easier for you to: Access, clean, and explore raw data Build data pipelines Run AI experiments Publish microservices Here is short visual represtion about DXC Indusrialized AI Starter.","title":"DXC Industrialized AI Starter"},{"location":"#installation","text":"In order to install and use the DXC AI Starter library, please use the below code snippet: 1. pip install DXC-Industrialized-AI-Starter 2. from dxc import ai","title":"Installation"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#access-clean-and-explore-raw-data","text":"Use the library to access, clean, and explore your raw data. #Access raw data df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) df = ai.read_data_frame_from_local_json() df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() #Clean data: Imputes missing data, removes empty rows and columns, anonymizes text. raw_data = ai.clean_dataframe(df) #Explore raw data: ai.visualize_missing_data(raw_data) #visualizes relationships between all features in data. ai.explore_features(raw_data) #creates a visual display of missing data. ai.plot_distributions(raw_data) #creates a distribution graph for each column. Click here for details about Acess,clean,explore raw data.","title":"Access, Clean, and Explore Raw Data"},{"location":"#build-data-pipelines","text":"Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. In order to get started, you need to first have an MongoDB account which you can signup for free and create a database \"connection_string\" and specify those details in the data_layer below. The following code connects to MongoDB and stores raw data for processing. #Insert data into MongoDB: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) Once raw data is stored, you can run pipelines to transform the data. This code instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) #refined data will be stored in pandas dataframe. Click here for details about building data pipeline.","title":"Build Data Pipelines"},{"location":"#run-ai-experiments","text":"Use the DXC AI Starter to build and test algorithms. This code executes an experiment by running run_experiment() on an experiment design. experiment_design = { #model options include ['regression()', 'classification()'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) Click here for details about run AI experiments.","title":"Run AI Experiments"},{"location":"#publish-microservice","text":"The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. You must create an Algorithmia account to use. Below is the example for publishing a microservice. #trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } #publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) Click here for details about publishing microservice.","title":"Publish Microservice"},{"location":"#example","text":"Here is an detailed and in-depth example of colab notebook for DXC Indusrialized AI Starter library usage.","title":"Example"},{"location":"#reporting-issues","text":"If you find any issues, feel free to report them here with clear description of your issue.","title":"Reporting Issues"},{"location":"access_clean/","text":"Access and Clean Access raw data Below code snippets help to bring in raw data from either a remote source or from your local machine into a dataframe. df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) Get raw data from your local machine by opening up a file system browser, identifying a file, and importing the selected file. df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() read_data_frame_from_remote_json(json_url): It reads JSON files from a URL and also flattened (in the case of nested data) the json data and cast into a pandas data frame. read_data_frame_from_remote_csv(csv_url, col_names = [], delim_whitespace=False, header = 'infer'): It allows you to read character-delimited (commas, tabs, spaces) data from a URL. Expect csv_url parameter remaining all are optional. read_data_frame_from_local_csv(col_names = [], delim_whitespace=False, header = 'infer'): This method allows you to import local character-delimited (commas, tabs, spaces) files. All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified. read_data_frame_from_local_excel_file(): This function allows you to import XLSX files. When the file explorer is launched, you must select an XLSX file or the function will result in an error. Clean raw data Below code snippet helps to clean raw data. raw_data = ai.clean_dataframe(df) clean_dataframe(df, impute = False, text_fields = [], date_fields = [], numeric_fields = [], categorical_fields = []): This method imputes missing data, cleans the column headings, removes empty rows and columns, anonymizes text, and casts fields to their proper data type. Except for the data, every input field for clean_dataframe is optional. By default the method will not impute missing data. If instructed, clean_dataframe will replace missing numeric fields with the mean value and replace missing categorical fields with the mode.","title":"Access and Clean"},{"location":"access_clean/#access-and-clean","text":"","title":"Access and Clean"},{"location":"access_clean/#access-raw-data","text":"Below code snippets help to bring in raw data from either a remote source or from your local machine into a dataframe. df = ai.read_data_frame_from_remote_json(json_url) df = ai.read_data_frame_from_remote_csv(csv_url) Get raw data from your local machine by opening up a file system browser, identifying a file, and importing the selected file. df = ai.read_data_frame_from_local_csv() df = ai.read_data_frame_from_local_excel_file() read_data_frame_from_remote_json(json_url): It reads JSON files from a URL and also flattened (in the case of nested data) the json data and cast into a pandas data frame. read_data_frame_from_remote_csv(csv_url, col_names = [], delim_whitespace=False, header = 'infer'): It allows you to read character-delimited (commas, tabs, spaces) data from a URL. Expect csv_url parameter remaining all are optional. read_data_frame_from_local_csv(col_names = [], delim_whitespace=False, header = 'infer'): This method allows you to import local character-delimited (commas, tabs, spaces) files. All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified. read_data_frame_from_local_excel_file(): This function allows you to import XLSX files. When the file explorer is launched, you must select an XLSX file or the function will result in an error.","title":"Access raw data"},{"location":"access_clean/#clean-raw-data","text":"Below code snippet helps to clean raw data. raw_data = ai.clean_dataframe(df) clean_dataframe(df, impute = False, text_fields = [], date_fields = [], numeric_fields = [], categorical_fields = []): This method imputes missing data, cleans the column headings, removes empty rows and columns, anonymizes text, and casts fields to their proper data type. Except for the data, every input field for clean_dataframe is optional. By default the method will not impute missing data. If instructed, clean_dataframe will replace missing numeric fields with the mean value and replace missing categorical fields with the mode.","title":"Clean raw data"},{"location":"contributing_guide/","text":"How to contribute We want to keep it as easy as possible to contribute changes that get things working in your environment. There are a few guidelines that we need contributors to follow so that we can have a chance of keeping on top of things. Getting Started Make sure you have a GitHub account . Submit a ticket for your issue, assuming one does not already exist. DXC Industrialized AI Starter tickets are filed on the GitHub project in issues tab . Please try to clearly describe the issue, including the steps to reproduce any bug. Please include the story of \"why\" you want to do something. Fork the repository on GitHub. Glance at the Git Best Practices document, even if you don't read it all yet. Making Changes Create a topic branch for your work. You should branch off the master branch. Name your branch by the type of contribution and target: Generally, the type is bug , or feature , but if they don't fit pick something sensible. To create a topic branch based on master: git checkout master && git pull && git checkout -b bug/master/my_contribution Don't work directly on the master branch, or any other core branch. Your pull request will be rejected unless it is on a topic branch. Every commit should do one thing, and only one thing. Having too many commits is better than having too few commits. Check for unnecessary whitespace with git diff --check before committing. Make sure your commit messages are in the proper format. If your commit fixes an issue, close it with your commit message (by appending, e.g., fixes #99999 , to the summary). Make sure you have added tests for your changes. Run all the tests to assure nothing else was accidentally broken. If possible, run the acceptance tests as well as the unit tests. You can always ask for help getting the tests working, or with writing tests. Click Here to view the basic test cases of AI starter library. Test cases file is located under Automate Test cases folder. These test cases should be passed by your new changes to ensure that existing basic functionalities are not affected by your latest commit. Please refer to Travis CI documentation for automate test case execution. Branching, and Where Changes Go Until a stable version of DXC Industrialized AI Starter is shipped, there is only one branch: master . All changes target that branch. Branch and Version Compatibility Any change to DXC Industrialized AI Starter branch should strive as much as possible to be compatible with all released versions of DXC Industrialized AI Starter. We want to avoid multiple incompatible versions existing as much as possible. Until 1.0.0 we are willing to accept backward-incompatible changes if there is no possible way around it. Those changes MUST provide a migration strategy and, if possible, deprecation warnings about the older functionality. Right now any change committed to master must be considered \"live\". Submitting Changes Unless your contribution is trivial , ensure you have signed the Contributor License Agreement . Push your changes to a topic branch in your fork of the repository. Submit a pull request to the repository in the Dxc organization. Update your ticket to mark that you have submitted code and are ready to be reviewed. Mentioning the issue number in the subject will make this happen through GitHub magic. A committer checks that the pull request is well formed. If not, they will ask that it is fixed: it is on its own, appropriately named, branch. it was submitted to an appropriate target branch. it only has commits relevant to the specific issue. it has appropriate, clear, and effective commit messages. A committer can start a pull request specific discussion; at this point that covers: Reviewing the code for any obvious problems. Providing feedback based on personal experience on the subject. Testing relevant examples on an untested platform. Thoroughly stepping through the change to understand potential side-effects. Examining discrepancies between the original issue and the pull request. Using @mentioning to involve another committer in the discussion. Anyone can offer their assessment of a pull request, or be involved in the discussion, but keep in mind that this isn't the time to decide if the pull request is desired or not. The only reason it should be rejected at this point is if someone skipped the earlier steps in the process and submitted code before any discussion happened. Every review should add any specific changes required to the pull request: For comments on specific code, using GitHub line comments. For general changes, include them in the final assessment. Every review should end by specifying the type of review, and a vote: Good to merge. Good to merge with minor changes (which are specified, or line comments). Not good to merge without major changes (which are specified). Any committer can merge after there is a vote of \"good to merge\". Committers are trusted to do the right thing - you can merge your own code, but you should make sure you get appropriate independent review. Most changes should not merge unless a code review has been completed. If the pull request is not reviewed within 14 days, you can ask any committer to execute the merge regardless: This can be blocked at any time by a single constructive vote against merging (\"Don't merge this, until you change...\") This is not stopped by a non-constructive vote (Don't merge this, I have not had a chance to look at it yet.\") The committer is encouraged to review before merging. Becoming a Committer DXC Industrialized AI Starter is an open project: any contributor can become a committer. Being a committer comes with great responsibility: your decisions directly shape the community, and the effectiveness, of the DXC Industrialized AI Starter project. You will probably invest more, and produce less, as a committer than a regular developer submitting pull requests. As a committer your code is subject to the same review and commit restrictions as regular committers. You must exercise greater caution that most people in what you submit and include in the project. On the other hand you have several additional responsibilities over and above those of a regular developer: 1. You are responsible for reviewing and voting on inclusion of code from other developers. * You are responsible for giving constructive feedback that action can be taken on when code isn't quite there yet 2. You are responsible for ensuring that quality, tested code is committed. 3. You are responsible for ensuring that code merges into the appropriate branch. 4. You are responsible for ensuring that our community is diverse, accepting, and friendly. 5. You are responsible for voting in a timely fashion, where required. The best way to become a committer is to fulfill those requirements in the community, so that it is clear that approving you is just a formality. The process for adding a committer is: A candidate has demonstrated familiarity with the quality guidelines and coding standards by submitting at least two pull requests that are accepted without modification. The candidate is proposed by an existing committer. A formal vote is held on the project private mailing list. Existing committers vote on the candidate: yes, accept them as a committer. no, do not accept them as a committer. If a majority of existing committers vote positively, the new committer is added to the public list of committers, and announced on the mailing list. Voting on adding a committer is absolutely private, and any feedback to candidates about why they were not accepted is at the option of the project leader. Removing Committers Removing a committer happens if they don't live up to their responsibilities, or if they violate the community standards. This is done by the project leader. The details of why are private, and will not be shared. Security issue notifications If you discover a potential security issue in this project we ask that you notify DXC Technology Security via email. Please do not create a public github issue","title":"How to Contribute"},{"location":"contributing_guide/#how-to-contribute","text":"We want to keep it as easy as possible to contribute changes that get things working in your environment. There are a few guidelines that we need contributors to follow so that we can have a chance of keeping on top of things.","title":"How to contribute"},{"location":"contributing_guide/#getting-started","text":"Make sure you have a GitHub account . Submit a ticket for your issue, assuming one does not already exist. DXC Industrialized AI Starter tickets are filed on the GitHub project in issues tab . Please try to clearly describe the issue, including the steps to reproduce any bug. Please include the story of \"why\" you want to do something. Fork the repository on GitHub. Glance at the Git Best Practices document, even if you don't read it all yet.","title":"Getting Started"},{"location":"contributing_guide/#making-changes","text":"Create a topic branch for your work. You should branch off the master branch. Name your branch by the type of contribution and target: Generally, the type is bug , or feature , but if they don't fit pick something sensible. To create a topic branch based on master: git checkout master && git pull && git checkout -b bug/master/my_contribution Don't work directly on the master branch, or any other core branch. Your pull request will be rejected unless it is on a topic branch. Every commit should do one thing, and only one thing. Having too many commits is better than having too few commits. Check for unnecessary whitespace with git diff --check before committing. Make sure your commit messages are in the proper format. If your commit fixes an issue, close it with your commit message (by appending, e.g., fixes #99999 , to the summary). Make sure you have added tests for your changes. Run all the tests to assure nothing else was accidentally broken. If possible, run the acceptance tests as well as the unit tests. You can always ask for help getting the tests working, or with writing tests. Click Here to view the basic test cases of AI starter library. Test cases file is located under Automate Test cases folder. These test cases should be passed by your new changes to ensure that existing basic functionalities are not affected by your latest commit. Please refer to Travis CI documentation for automate test case execution.","title":"Making Changes"},{"location":"contributing_guide/#branching-and-where-changes-go","text":"Until a stable version of DXC Industrialized AI Starter is shipped, there is only one branch: master . All changes target that branch.","title":"Branching, and Where Changes Go"},{"location":"contributing_guide/#branch-and-version-compatibility","text":"Any change to DXC Industrialized AI Starter branch should strive as much as possible to be compatible with all released versions of DXC Industrialized AI Starter. We want to avoid multiple incompatible versions existing as much as possible. Until 1.0.0 we are willing to accept backward-incompatible changes if there is no possible way around it. Those changes MUST provide a migration strategy and, if possible, deprecation warnings about the older functionality. Right now any change committed to master must be considered \"live\".","title":"Branch and Version Compatibility"},{"location":"contributing_guide/#submitting-changes","text":"Unless your contribution is trivial , ensure you have signed the Contributor License Agreement . Push your changes to a topic branch in your fork of the repository. Submit a pull request to the repository in the Dxc organization. Update your ticket to mark that you have submitted code and are ready to be reviewed. Mentioning the issue number in the subject will make this happen through GitHub magic. A committer checks that the pull request is well formed. If not, they will ask that it is fixed: it is on its own, appropriately named, branch. it was submitted to an appropriate target branch. it only has commits relevant to the specific issue. it has appropriate, clear, and effective commit messages. A committer can start a pull request specific discussion; at this point that covers: Reviewing the code for any obvious problems. Providing feedback based on personal experience on the subject. Testing relevant examples on an untested platform. Thoroughly stepping through the change to understand potential side-effects. Examining discrepancies between the original issue and the pull request. Using @mentioning to involve another committer in the discussion. Anyone can offer their assessment of a pull request, or be involved in the discussion, but keep in mind that this isn't the time to decide if the pull request is desired or not. The only reason it should be rejected at this point is if someone skipped the earlier steps in the process and submitted code before any discussion happened. Every review should add any specific changes required to the pull request: For comments on specific code, using GitHub line comments. For general changes, include them in the final assessment. Every review should end by specifying the type of review, and a vote: Good to merge. Good to merge with minor changes (which are specified, or line comments). Not good to merge without major changes (which are specified). Any committer can merge after there is a vote of \"good to merge\". Committers are trusted to do the right thing - you can merge your own code, but you should make sure you get appropriate independent review. Most changes should not merge unless a code review has been completed. If the pull request is not reviewed within 14 days, you can ask any committer to execute the merge regardless: This can be blocked at any time by a single constructive vote against merging (\"Don't merge this, until you change...\") This is not stopped by a non-constructive vote (Don't merge this, I have not had a chance to look at it yet.\") The committer is encouraged to review before merging.","title":"Submitting Changes"},{"location":"contributing_guide/#becoming-a-committer","text":"DXC Industrialized AI Starter is an open project: any contributor can become a committer. Being a committer comes with great responsibility: your decisions directly shape the community, and the effectiveness, of the DXC Industrialized AI Starter project. You will probably invest more, and produce less, as a committer than a regular developer submitting pull requests. As a committer your code is subject to the same review and commit restrictions as regular committers. You must exercise greater caution that most people in what you submit and include in the project. On the other hand you have several additional responsibilities over and above those of a regular developer: 1. You are responsible for reviewing and voting on inclusion of code from other developers. * You are responsible for giving constructive feedback that action can be taken on when code isn't quite there yet 2. You are responsible for ensuring that quality, tested code is committed. 3. You are responsible for ensuring that code merges into the appropriate branch. 4. You are responsible for ensuring that our community is diverse, accepting, and friendly. 5. You are responsible for voting in a timely fashion, where required. The best way to become a committer is to fulfill those requirements in the community, so that it is clear that approving you is just a formality. The process for adding a committer is: A candidate has demonstrated familiarity with the quality guidelines and coding standards by submitting at least two pull requests that are accepted without modification. The candidate is proposed by an existing committer. A formal vote is held on the project private mailing list. Existing committers vote on the candidate: yes, accept them as a committer. no, do not accept them as a committer. If a majority of existing committers vote positively, the new committer is added to the public list of committers, and announced on the mailing list. Voting on adding a committer is absolutely private, and any feedback to candidates about why they were not accepted is at the option of the project leader.","title":"Becoming a Committer"},{"location":"contributing_guide/#removing-committers","text":"Removing a committer happens if they don't live up to their responsibilities, or if they violate the community standards. This is done by the project leader. The details of why are private, and will not be shared.","title":"Removing Committers"},{"location":"contributing_guide/#security-issue-notifications","text":"If you discover a potential security issue in this project we ask that you notify DXC Technology Security via email. Please do not create a public github issue","title":"Security issue notifications"},{"location":"data_pipeline/","text":"Build Data Pipelines Insert data into MongoDB Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. This code defines the meta-data needed to connect to Mongo DB Atlas and create a new data store cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. In order to provide the information required in data_layer, you must: Create a MongoDb Atlas account Create a cluster Create a user Generate a connection string Note: When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. When creating the database connection string, choose the Python driver version 3.4 or later. Example: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) write_raw_data(data_layer, raw_data, date_fields = [ ]): This function handles Mongo DB Atlas automatically. Use write_raw_data function from ai library to convert Arrow dates to Strings data types It also transfers the raw data into the database and collection. Build Pipeline Once raw data is stored, you can run pipelines to transform the data. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) access_data_from_pipeline(write_raw_data, pipeline): This function instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. The refined data will be stored in the df Pandas dataframe. Make sure the output is what you want before continuing.","title":"Data Pipeline"},{"location":"data_pipeline/#build-data-pipelines","text":"","title":"Build Data Pipelines"},{"location":"data_pipeline/#insert-data-into-mongodb","text":"Pipelines are a standard way to process your data towards modeling and interpreting. By default, the DXC AI Starter library uses the free tier of MongoDB Atlas to store raw data and execute pipelines. This code defines the meta-data needed to connect to Mongo DB Atlas and create a new data store cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. In order to provide the information required in data_layer, you must: Create a MongoDb Atlas account Create a cluster Create a user Generate a connection string Note: When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. When creating the database connection string, choose the Python driver version 3.4 or later. Example: data_layer = { \"connection_string\": \"<your connection_string>\", \"collection_name\": \"<your collection_name>\", \"database_name\": \"<your database_name>\" } wrt_raw_data = ai.write_raw_data(data_layer, raw_data, date_fields = []) write_raw_data(data_layer, raw_data, date_fields = [ ]): This function handles Mongo DB Atlas automatically. Use write_raw_data function from ai library to convert Arrow dates to Strings data types It also transfers the raw data into the database and collection.","title":"Insert data into MongoDB"},{"location":"data_pipeline/#build-pipeline","text":"Once raw data is stored, you can run pipelines to transform the data. Please refer to the syntax of MongDB pipelines for the details of how to write a pipeline. Below is an example of creating and executing a pipeline. pipeline = [ { '$group':{ '_id': { \"funding_source\":\"$funding_source\", \"request_type\":\"$request_type\", \"department_name\":\"$department_name\", \"replacement_body_style\":\"$replacement_body_style\", \"equipment_class\":\"$equipment_class\", \"replacement_make\":\"$replacement_make\", \"replacement_model\":\"$replacement_model\", \"procurement_plan\":\"$procurement_plan\" }, \"avg_est_unit_cost\":{\"$avg\":\"$est_unit_cost\"}, \"avg_est_unit_cost_error\":{\"$avg\":{ \"$subtract\": [ \"$est_unit_cost\", \"$actual_unit_cost\" ] }} } } ] df = ai.access_data_from_pipeline(wrt_raw_data, pipeline) access_data_from_pipeline(write_raw_data, pipeline): This function instructs the data store on how to refine the output of raw data into something that can be used to train a machine-learning model. The refined data will be stored in the df Pandas dataframe. Make sure the output is what you want before continuing.","title":"Build Pipeline"},{"location":"data_sets/","text":"Load Data Finding right or sample data set is one of the important tasks for those who are starting to work on Machine learning and AI. Though we have many datesets readily available finding right dataset, loading and using it can always be challangeing specically for beginners. DXC Industrialized AI Starter make this easy by providing few data sets and functions to load them easily without any difficulties. Below code sinpet helps you to load and view the datsets from this library. # To know more details about the dataset print(ai.load_data_details('bike_sharing_data')) # To load the data sets into pandas dataframe df = ai.load_data('bike_sharing_data') df.head() For loading the datasets from this library we use below two functions. ai.load_data(filename): It takes filename as input and loads that particular datset into your notebook and return a pandas dataframe. ai.load_data_details(filename): This function loads the dataset details and print information. This function takes filename as input. The information retrieved helps user to understand the dataset and significance of those columns in dataset. List of datasets available in AI Starter are provided below. Example note on how to use this functions is placed under \u201cExamples\u201d folder in git repository. Available Datasets Below datasets are availabe in DXC Industrialized AI Starter library. Dataset Name Filename to load Abalone data abalone","title":"Load Data"},{"location":"data_sets/#load-data","text":"Finding right or sample data set is one of the important tasks for those who are starting to work on Machine learning and AI. Though we have many datesets readily available finding right dataset, loading and using it can always be challangeing specically for beginners. DXC Industrialized AI Starter make this easy by providing few data sets and functions to load them easily without any difficulties. Below code sinpet helps you to load and view the datsets from this library. # To know more details about the dataset print(ai.load_data_details('bike_sharing_data')) # To load the data sets into pandas dataframe df = ai.load_data('bike_sharing_data') df.head() For loading the datasets from this library we use below two functions. ai.load_data(filename): It takes filename as input and loads that particular datset into your notebook and return a pandas dataframe. ai.load_data_details(filename): This function loads the dataset details and print information. This function takes filename as input. The information retrieved helps user to understand the dataset and significance of those columns in dataset. List of datasets available in AI Starter are provided below. Example note on how to use this functions is placed under \u201cExamples\u201d folder in git repository.","title":"Load Data"},{"location":"data_sets/#available-datasets","text":"Below datasets are availabe in DXC Industrialized AI Starter library. Dataset Name Filename to load Abalone data abalone","title":"Available Datasets"},{"location":"experiment/","text":"Run AI Experiments An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance. Below is a example for run_experiment() function. experiment_design = { #model options include ['regression()', 'classification()', 'timeseries'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) run_experiment(experiment_design): This function executes an experiment on selected model. Update experiment_design with parameters that fit your project. The data parameter should be same as the refined training data. The model parameter must be a model subclass. The labels parameter indicates the column of the data dataframe to be predicted. For the prediction model, the meta-data must describe the column to be predicted and the types for non-numeric columns. Further Information on Model Machine learning model provides the intelligent behavior that you will publish as a microservice. The code in this section provides you with options for the model. You must select a model capable of using df to learn the behavior specified in the design section of the datastory. Run this function by defining each model type, then choose the model most appropriate for your data story. Each model adheres to the specifications of a model. This allows any of the models to run according to the standard model lifecycle defined in run_experiment. Regressor model: The regressor model makes a numeric prediction. Use this model when the design specification of the data story requires the AI microservice to give a numerical output prediction. Classification model: The classification model makes a classification prediction. Use this model when the design specification of the data story requires the AI microservice to give a categorical (text-based) output prediction. Timeseries model: The timeseries model automated the process for predicting future values of a signal using a machine learning approach. It allows forecasting a time series (or a signal) for future values in a fully automated way. Use this model when design specification of the data story requires to predict future values based on time. Prediction: This section defines a new type of model by creating a subclass of model. The prediction model learns to predict a particular outcome. It automatically optimizes parameters, selects features, selects an algorithm, and scores the results. Deep Learning Model DXC Industrialized AI starter has Deep Learning model for image classification. Image classification is a supervised learning problem. A set of target classes are defined, and a model is trained with labeled images (train set). This trained model will identify and classifies new input into one of the target class. This model can be used through three functions mentioned below ai.create_training_data ai.split_normalize_data ai.image_classifier ai.create_training_data: This function reads each folder and each image from the path provided and will convert image to an array. It will also resize your image to the size provided, default size is 100. This function takes list of image categories, image folder directory/path and resize size. This function return features and labels. ai.split_normalize_data: This function split your training data into train and test based on the size of test provided. Default test size will be 0.20. Along with splitting the data to train and test, this function will also normalize the data.Th is function takes features, labels, test size (default 0.20), category count and image size (default 100).This function returns train and test sets. ai.image_classifier: This function compiles the model with the image size and number of categories provided. It adds the input, hidden and output layers and compiles the model. Here returned model is trained with train data sets and further used for prediction. An example notebook for this model is placed under \u201cExamples\u201d folder in git repository.","title":"Experiment"},{"location":"experiment/#run-ai-experiments","text":"An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance. Below is a example for run_experiment() function. experiment_design = { #model options include ['regression()', 'classification()', 'timeseries'] \"model\": ai.regression(), \"labels\": df.avg_est_unit_cost_error, \"data\": df, #Tell the model which column is 'output' #Also note columns that aren't purely numerical #Examples include ['nlp', 'date', 'categorical', 'ignore'] \"meta_data\": { \"avg_est_unit_cost_error\": \"output\", \"_id.funding_source\": \"categorical\", \"_id.department_name\": \"categorical\", \"_id.replacement_body_style\": \"categorical\", \"_id.replacement_make\": \"categorical\", \"_id.replacement_model\": \"categorical\", \"_id.procurement_plan\": \"categorical\" } } trained_model = ai.run_experiment(experiment_design) run_experiment(experiment_design): This function executes an experiment on selected model. Update experiment_design with parameters that fit your project. The data parameter should be same as the refined training data. The model parameter must be a model subclass. The labels parameter indicates the column of the data dataframe to be predicted. For the prediction model, the meta-data must describe the column to be predicted and the types for non-numeric columns. Further Information on Model Machine learning model provides the intelligent behavior that you will publish as a microservice. The code in this section provides you with options for the model. You must select a model capable of using df to learn the behavior specified in the design section of the datastory. Run this function by defining each model type, then choose the model most appropriate for your data story. Each model adheres to the specifications of a model. This allows any of the models to run according to the standard model lifecycle defined in run_experiment. Regressor model: The regressor model makes a numeric prediction. Use this model when the design specification of the data story requires the AI microservice to give a numerical output prediction. Classification model: The classification model makes a classification prediction. Use this model when the design specification of the data story requires the AI microservice to give a categorical (text-based) output prediction. Timeseries model: The timeseries model automated the process for predicting future values of a signal using a machine learning approach. It allows forecasting a time series (or a signal) for future values in a fully automated way. Use this model when design specification of the data story requires to predict future values based on time. Prediction: This section defines a new type of model by creating a subclass of model. The prediction model learns to predict a particular outcome. It automatically optimizes parameters, selects features, selects an algorithm, and scores the results.","title":"Run AI Experiments"},{"location":"experiment/#deep-learning-model","text":"DXC Industrialized AI starter has Deep Learning model for image classification. Image classification is a supervised learning problem. A set of target classes are defined, and a model is trained with labeled images (train set). This trained model will identify and classifies new input into one of the target class. This model can be used through three functions mentioned below ai.create_training_data ai.split_normalize_data ai.image_classifier ai.create_training_data: This function reads each folder and each image from the path provided and will convert image to an array. It will also resize your image to the size provided, default size is 100. This function takes list of image categories, image folder directory/path and resize size. This function return features and labels. ai.split_normalize_data: This function split your training data into train and test based on the size of test provided. Default test size will be 0.20. Along with splitting the data to train and test, this function will also normalize the data.Th is function takes features, labels, test size (default 0.20), category count and image size (default 100).This function returns train and test sets. ai.image_classifier: This function compiles the model with the image size and number of categories provided. It adds the input, hidden and output layers and compiles the model. Here returned model is trained with train data sets and further used for prediction. An example notebook for this model is placed under \u201cExamples\u201d folder in git repository.","title":"Deep Learning Model"},{"location":"explore/","text":"Explore Data Explore raw data Below code snippet helps to explore and visualize raw data. ai.visualize_missing_data(raw_data) ai.explore_features(raw_data) ai.plot_distributions(raw_data) explore_features(df): It visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction. It is useful to explore the correlations between features in the raw data. Use this visualization to form a hypothesis about how the raw data can be used. It may be necessary to enrich raw data with other features to increase the number and strength of correlations. visualize_missing_data(df): It creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis. It is useful to visualize missing fields in your raw data. Determine if imputing is necessary. plot_distributions(df): It creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data. Use plot_distributions to show the distributions for each feature in raw_data. Depending on raw data, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight. Visualize complete data: report = ai.explore_complete_data(df) report.to_notebook_iframe() ai.explore_complete_data(df): This function generates a profile reports from a pandas DataFrame for exploratory data analysis. For each column the following statistics are presented in an interactive HTML report. You can download that as an HTML report and share to others easily. Type inference: detect the types of columns in a DataFrame. Essentials: type, unique values, missing values Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness Most frequent values Histogram Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices Missing values matrix, count, heatmap and dendrogram of missing values Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data. File and Image analysis extract file sizes, creation dates and dimensions and scan for truncated images or those containing EXIF information.","title":"Visualization"},{"location":"explore/#explore-data","text":"","title":"Explore Data"},{"location":"explore/#explore-raw-data","text":"Below code snippet helps to explore and visualize raw data. ai.visualize_missing_data(raw_data) ai.explore_features(raw_data) ai.plot_distributions(raw_data) explore_features(df): It visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction. It is useful to explore the correlations between features in the raw data. Use this visualization to form a hypothesis about how the raw data can be used. It may be necessary to enrich raw data with other features to increase the number and strength of correlations. visualize_missing_data(df): It creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis. It is useful to visualize missing fields in your raw data. Determine if imputing is necessary. plot_distributions(df): It creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data. Use plot_distributions to show the distributions for each feature in raw_data. Depending on raw data, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight.","title":"Explore raw data"},{"location":"explore/#visualize-complete-data","text":"report = ai.explore_complete_data(df) report.to_notebook_iframe() ai.explore_complete_data(df): This function generates a profile reports from a pandas DataFrame for exploratory data analysis. For each column the following statistics are presented in an interactive HTML report. You can download that as an HTML report and share to others easily. Type inference: detect the types of columns in a DataFrame. Essentials: type, unique values, missing values Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness Most frequent values Histogram Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices Missing values matrix, count, heatmap and dendrogram of missing values Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data. File and Image analysis extract file sizes, creation dates and dimensions and scan for truncated images or those containing EXIF information.","title":"Visualize complete data:"},{"location":"logging/","text":"Logging Functionality Logging is an important module in tracking the work or performance. Maintaining logs helps is tracing the error, variable value and other many uses. DXC industrialized AI starter also performs default logging for few crucial functions. We have provided the automatic logging functionality for below major funtions in this library. Build Data Pipelines Run AI Experiments Publish Microservice These auto logging is introduced to save the user build pipelines, inputs provided for run experiment function and also to store the microservice design skeleton used for publishing model in algorithmia. These logs can be referred back in case to rerun or reuse any pipeline or microservice design details. Logs will be saved in the same location where notebook with functions used. Refer to sample example notebook for reference of sample logs.","title":"Logging"},{"location":"logging/#logging-functionality","text":"Logging is an important module in tracking the work or performance. Maintaining logs helps is tracing the error, variable value and other many uses. DXC industrialized AI starter also performs default logging for few crucial functions. We have provided the automatic logging functionality for below major funtions in this library. Build Data Pipelines Run AI Experiments Publish Microservice These auto logging is introduced to save the user build pipelines, inputs provided for run experiment function and also to store the microservice design skeleton used for publishing model in algorithmia. These logs can be referred back in case to rerun or reuse any pipeline or microservice design details. Logs will be saved in the same location where notebook with functions used. Refer to sample example notebook for reference of sample logs.","title":"Logging Functionality"},{"location":"model_explainability/","text":"Model Explainability In the context of machine learning and artificial intelligence, Explainability is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. Model explainability is one of the most important problems in machine learning today. DXC Industrialized AI Starter helps you to understand the model explainability using interactive dashboards via SHAP - based explainer. global_explanation = ai.Global_Model_Explanation(model,x_train,x_test,feature_names = None,classes = None, explantion_data = None) ai.Explanation_Dashboard(global_explanation, model, x_train, x_test, explantion_data = None) To generate the model explainability, you need to pass your model, training data, test data to the functions. You can also optionally pass in feature names and output class names(classification) which will be used to make the explanations and visualizations more informative. Explanations will be generated default on the test data. If you pass the value of explantion_data parameter as 'Training', then the explanation will be generated on training data. But with more examples, explanations will take longer although they may be more accurate. ai.Global_Model_Explanation: This function generates the overall model predictions and generates a dictionary of sorted feature importance names and values. ai.Explanation_Dashboard: This will generate an interactive visualization dashboard, you can investigate different aspects of your dataset and trained model via below four tab views: Model Performance Data Explorer Aggregate Feature Importance Individual Feature Importance SHAP explainer SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. Depending on the model, Model Explainer uses one of the below supported SHAP explainers. SHAP TreeExplainer SHAP DeepExplainer SHAP LinearExplainer SHAP KernelExplainer To know more details about SHAP explainer click here . Check out Examples to understand how to use each function, what parameters are expected for each function. Also check out shap , lime , interpret-community libraries to learn more about the Model explainability and its usage. Note: In the present version, Model Explainability supports only the custom models. We are implementing the explainability changes for Auto_ml models. Changes will be published soon.","title":"Model Explainability"},{"location":"model_explainability/#model-explainability","text":"In the context of machine learning and artificial intelligence, Explainability is the extent to which the internal mechanics of a machine or deep learning system can be explained in human terms. Model explainability is one of the most important problems in machine learning today. DXC Industrialized AI Starter helps you to understand the model explainability using interactive dashboards via SHAP - based explainer. global_explanation = ai.Global_Model_Explanation(model,x_train,x_test,feature_names = None,classes = None, explantion_data = None) ai.Explanation_Dashboard(global_explanation, model, x_train, x_test, explantion_data = None) To generate the model explainability, you need to pass your model, training data, test data to the functions. You can also optionally pass in feature names and output class names(classification) which will be used to make the explanations and visualizations more informative. Explanations will be generated default on the test data. If you pass the value of explantion_data parameter as 'Training', then the explanation will be generated on training data. But with more examples, explanations will take longer although they may be more accurate. ai.Global_Model_Explanation: This function generates the overall model predictions and generates a dictionary of sorted feature importance names and values. ai.Explanation_Dashboard: This will generate an interactive visualization dashboard, you can investigate different aspects of your dataset and trained model via below four tab views: Model Performance Data Explorer Aggregate Feature Importance Individual Feature Importance","title":"Model Explainability"},{"location":"model_explainability/#shap-explainer","text":"SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions. Depending on the model, Model Explainer uses one of the below supported SHAP explainers. SHAP TreeExplainer SHAP DeepExplainer SHAP LinearExplainer SHAP KernelExplainer To know more details about SHAP explainer click here . Check out Examples to understand how to use each function, what parameters are expected for each function. Also check out shap , lime , interpret-community libraries to learn more about the Model explainability and its usage. Note: In the present version, Model Explainability supports only the custom models. We are implementing the explainability changes for Auto_ml models. Changes will be published soon.","title":"SHAP explainer"},{"location":"publish_microservice/","text":"Publish Microservice The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. In order to provide the information required to design the microservice, you must: create an Algorithmia account create an API key with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled create an algorithm user name trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } # publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) publish_microservice(microservice_design, trained_model): The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API enpoint for the microservice. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code reposity management. This code defines the parameters needed to build and delpoy a microservice based on the trained model. Update microservice_design with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia.","title":"Publish Microservice"},{"location":"publish_microservice/#publish-microservice","text":"The DXC AI Starter library makes it easy to publish your models as working microservices. By default, the DXC AI Starter library uses free tier of Algorithmia to publish models as microservices. In order to provide the information required to design the microservice, you must: create an Algorithmia account create an API key with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled create an algorithm user name trained_model is the output of run_experiment() function microservice_design = { \"microservice_name\": \"<Name of your microservice>\", \"microservice_description\": \"<Brief description about your microservice>\", \"execution_environment_username\": \"<Algorithmia username>\", \"api_key\": \"<your api_key>\", \"api_namespace\": \"<your api namespace>\", \"model_path\":\"<your model_path>\" } # publish the micro service and display the url of the api api_url = ai.publish_microservice(microservice_design, trained_model) print(\"api url: \" + api_url) publish_microservice(microservice_design, trained_model): The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API enpoint for the microservice. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code reposity management. This code defines the parameters needed to build and delpoy a microservice based on the trained model. Update microservice_design with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia.","title":"Publish Microservice"}]}